### Tensorflow + Keras

### Export TF 

Note that the two first steps are the same as when we *load any graph in TF*, 
the only tricky part is actually the graph *freezing* and TF has a built-in function to do it!
La fonction *freeze_graph()* d'origine fournie par TF est installée dans votre binrépertoire et peut être appelée directement si vous avez utilisé PIP pour installer TF


## Keras

There are different ways to save TensorFlow models—depending on the API you're using. 
This guide uses *tf.keras*, *a high-level API to build and train models in TensorFlow.* 
For other approaches, see the TensorFlow Save and Restore guide or Saving in eager.

Keras is a *high-level neural networks API*, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. 

It was developed with a focus on enabling fast experimentation. 

Being able to go from idea to result with the least possible delay is key to doing good research.


Use Keras if you need a deep learning library that:

+ Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).
+ Supports both convolutional networks and recurrent networks, as well as combinations of the two.
+ Runs seamlessly on CPU and GPU.

## Guiding principles


+ *User friendliness* Keras is an API designed for *human beings, not machines*. 
It puts user experience front and center. *Keras follows best practices* for reducing cognitive load: it offers consistent & simple APIs, *it minimizes the number of user actions required* for common use cases, and it provides clear and actionable feedback upon user error.

+ *Modularity* A model is understood as a sequence or a graph of standalone, fully-configurable modules that can be plugged together with as few restrictions as possible. In particular, *neural layers, cost functions, optimizers, initialization schemes, activation functions*, regularization schemes are all standalone modules that you can combine to create new models.

+ *Easy extensibility* New modules are simple to add (as new classes and functions), and existing modules provide ample examples. To be able to easily create new modules allows for total expressiveness, making Keras suitable for advanced research.

+ *Work with Python* No separate models configuration files in a declarative format. Models are described in Python code, which is compact, easier to debug, and allows for ease of extensibility.


### How to freeze (export) a saved model

Let’s start from a folder containing a model, it probably looks something like this:


[https://cdn-images-1.medium.com/max/1600/1*lrMvfG1uOeHr3IGMY77LQA.png](https://cdn-images-1.medium.com/max/1600/1*lrMvfG1uOeHr3IGMY77LQA.png)



The important files here are the “.chkp” ones.


[https://www.tensorflow.org/tutorials/keras/save_and_restore_models]
(https://www.tensorflow.org/tutorials/keras/save_and_restore_models)



## Save checkpoints during training

tf.keras.callbacks.ModelCheckpoint is a callback that performs this task. The callback takes a couple of arguments to configure checkpointing.

### Checkpoint callback usage

```
checkpoint_path = "training_1/cp.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)

# Create checkpoint callback
cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, 
                                                 save_weights_only=True,
                                                 verbose=1)

model = create_model()

model.fit(train_images, train_labels,  epochs = 10, 
          validation_data = (test_images,test_labels),
          callbacks = [cp_callback])  # pass callback to training

```


This creates a single collection of TensorFlow checkpoint files that are updated at the end of each epoch:

```
    !ls {checkpoint_dir}
    checkpoint  cp.ckpt.data-00000-of-00001  cp.ckpt.index
```

Create a *new, untrained model*. When restoring a model from only weights, you must have a model with *the same architecture as the original model*. 
Since it's the same model architecture, we can *share weights* despite that it's a different instance of the model.

Now rebuild a fresh, untrained model, and evaluate it on the test set. An untrained model will perform at chance levels (~10% accuracy):


Epoch

```
Train on 1000 samples, validate on 1000 samples
Epoch 1/10
 608/1000 [=================>............] - ETA: 0s - loss: 1.5240 - acc: 0.5115
Epoch 00001: saving model to training_1/cp.ckpt
```


The important files here are the “.chkp” ones. If you remember well, for each pair at different timesteps, one is holding the weights (“.data”) and the other one (“.meta”) is holding the graph and all its metadata (so you can retrain it etc…)


### Checkpoint callback options

The callback provides several options to give the resulting checkpoints unique names, and adjust the checkpointing frequency.

Train a new model, and save uniquely named checkpoints once every 5-epochs:

```
    # include the epoch in the file name. (uses `str.format`)
    checkpoint_path = "training_2/cp-{epoch:04d}.ckpt"
    checkpoint_dir = os.path.dirname(checkpoint_path)

    cp_callback = tf.keras.callbacks.ModelCheckpoint(
        checkpoint_path, verbose=1, save_weights_only=True,
        # Save weights, every 5-epochs.
        period=5)

    model = create_model()
    model.fit(train_images, train_labels,
            epochs = 50, callbacks = [cp_callback],
            validation_data = (test_images,test_labels),
            verbose=0)
```


### Manually save weights

Dossier: /checkpoints
```
# Save the weights
model.save_weights('./checkpoints/my_checkpoint')

# Restore the weights
model = create_model()
model.load_weights('./checkpoints/my_checkpoint')

loss,acc = model.evaluate(test_images, test_labels)
print("Restored model, accuracy: {:5.2f}%".format(100*acc))
```


### Save all Model

# Keras provides a basic save format using the HDF5 standard. For our purposes, the saved model can be treated as a single binary blob.


```
    model = create_model()
    # You need to use a keras.optimizer to restore the optimizer state from an HDF5 file.
    model.compile(optimizer=keras.optimizers.Adam(), 
                loss=tf.keras.losses.sparse_categorical_crossentropy,
                metrics=['accuracy'])
    model.fit(train_images, train_labels, epochs=5)
    # Save entire model to a HDF5 file
    model.save('my_model.h5')
```


### Download the IMDB dataset


Rather than using an embedding as in the previous notebook, here we will multi-hot encode the sentences. This model will quickly overfit to the training set. It will be used to demonstrate when overfitting occurs, and how to fight it.

```
NUM_WORDS = 10000

(train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=NUM_WORDS)

def multi_hot_sequences(sequences, dimension):
    # Create an all-zero matrix of shape (len(sequences), dimension)
    results = np.zeros((len(sequences), dimension))
    for i, word_indices in enumerate(sequences):
        results[i, word_indices] = 1.0  # set specific indices of results[i] to 1s
    return results


train_data = multi_hot_sequences(train_data, dimension=NUM_WORDS)
test_data = multi_hot_sequences(test_data, dimension=NUM_WORDS)
```
